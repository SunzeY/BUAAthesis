% !Mode:: "TeX:UTF-8"
\chapter{论文解决的问题}
\section{梯度掩盖和极值退化现象的解释}
深度学习模型应对扰动攻击表现脆弱的原因，在前人的工作中被猜想为“极值退化”和“梯度掩盖”，但现有验证性的实验较少。对抗训练对模型的
影响也缺少实验论证。

\section{现有扰动攻击的局限性}
目前扰动攻击方法的方法有很多，已在1.2 常见攻击手段进行了简要介绍。其中，许多扰动攻击方案或需要人为的构造(Deep Fool、单像素攻击等)，
或需要大量的计算开销(k-FGSM、n-Step-LL等)，大多不能应用于对抗训练。而FGSM方法和与之类似的Step-LL方法被广泛应用于对抗训练\cite{DBLP:conf/iclr/KurakinGB17}。
\begin{equation}
    x_{FGSM}^{adv} = x + \varepsilon \cdot sign(\nabla_x L(h(x), y_{true}))  
    \label{FGSM}
\end{equation}
FGSM扰动攻击生成公式\ref{FGSM}中，$x$为输入feature，$L$为损失函数，$y_{true}$为正确标签，$h$为网络模型。
这种产生模型扰动的方法基于当前模型的梯度计算(即线性近似)，而由于机器学习系统的非线性特性和复杂性，如果在扰动点处模型的曲率过大，会
产生“梯度掩盖”的效果，导致扰动的结果退化为局部而非全局最优，对抗训练的效果大大减弱。

\section{现有对抗训练的局限性}
现有的对抗训练方法中，对抗样本的产生均产生自当前进行训练的模型。经过对抗训练的模型对于白盒攻击具有较好的鲁棒性，但由于扰动攻击的
可转移性，这些模型在应对现实中更加广泛存在的黑盒攻击往往表现脆弱。